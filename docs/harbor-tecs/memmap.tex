%===========================================================
% MEMORY MAP SECTION
%===========================================================
\section{Memory Map Manager}
\label{sec:memmap}
%
%\subsection{SOS Operating System}
%\label{sec:background}
% 
%The initial motivation of our work was to provide memory protection
%for mote-class sensor nodes running the SOS operating system.
% 
% The design ideas of Harbor are generally applicable however its
% implementation and evaluation are specific to SOS.
% 
%We have implemented and evaluated Harbor's protection mechanisms on
%SOS, although the ideas should apply elsewhere.
% 
%This brief introduction to SOS is useful to fully understand the
%implementation of our scheme.
% 
%===================================================
%\subsection{SOS Operating System}
% 
%% Unique resource tradeoffs in the nodes, the distributed and
%% collaborative nature of applications and the remote unattended
%% operation of sensor networks motivate the design of a new class of
%% operating system and run-times.
% 
%TinyOS~\cite{levis05t2}, the most popular operating system for sensor
%networks, uses reusable software components to implement common
%services, but each node runs a statically linked system image.
% 
%% The components are written in the NesC~\cite{gay03nesc} programming
%% language.
% 
%% Mat\'e~\cite{asvm05nsdi}, an application specific virtual machine
%% on top of TinyOS provides limited flexibility to re-task a deployed
%% network using high-level scripts.
% 
%SOS has a traditional operating system architecture: a kernel is
%installed on all nodes, and
% 
% The rest of the system and 
%application level functionality is implemented by a set of dynamically
%loadable binary modules~\cite{ram05sos}.
% 
% In contrast to TinyOS, SOS maintains modularity at the binary level
% without any loss in efficiency.
% 
% This property of SOS makes it particularly attractive to explore
% techniques for isolating software faults.
% 
%% We have implemented the memory protection mechanism on the SOS
%% operating system.
% 
%% The embodiment of the scheme is specific to SOS, however it can
%% easily be applied to the other existing run-times as well
%% (subsection~\ref{sec:conclude}).
% 
% any operating system that supports dynamically loadable binary
% modules.
% 
% This brief introduction to SOS is useful to fully understand the
% implementation of our scheme.
% 
% 
% 
%The kernel is relatively well tested; we assume it is free of
%programming errors.
% 
%Modules are position independent binaries that implement a specific
%task or function, and are less well tested by comparison.
% 
%Modules operate on their own state, which is dynamically allocated at
%run-time.
% 
%An application in SOS is composed of one or more modules interacting
%via asynchronous messages or function calls.
% 
%Examples of modules are routing protocols, sensor drivers, application
%programs, and so forth.
%=================================================================
\subsection{Protection Domains}
% 
In SOS, user modules' wild writes can easily corrupt kernel state
and trigger severe failure conditions.
% 
Harbor aims to prevent this systemwide corruption through \emph{protection domains}.
% 
%preventing user modules
%from corrupting memory in different
% 
% We propose an approach to provide software based memory protection
% and implement it in SOS operating system.
% 
These protection domains are
% 
distinct subsets of a sensor node's overall data memory address space
(Figure~\ref{fig:prot_domains}), created and enforced by Harbor.
% 
Each module resides in exactly one domain; SOS kernel state resides in
its own, separate domain.
% 
The kernel can read and write any domain, but each module can only write
into its own domain.
% 
(This simple design precludes writable shared memory regions, but SOS did
not previously support such regions anyway.)
% 
The number of protection domains is subject to a tradeoff between space
efficiency and fault protection.
%
Harbor's space overhead is minimized when there are two domains, one
for the kernel and one for all user modules.
%
This protects kernel state from user wild writes, but allows any
module to corrupt any other module's state.
%
Alternatively, each module might store its state in a separate
protection domain.
% 
No assumptions are made about layout of state within a domain.
% 
Run-time checks restrict user modules from writing to memory outside
their domains.
% 
Checks are added to each memory write, and to jumps and other control
flow transfers.  
%
The control flow checks prevent applications from avoiding the former memory access
checks.
%================================================================
\begin{figure}[htbp]
  \centering
  \includegraphics[height = 0.75in,
  keepaspectratio=true]{figures/domains.eps} 
  \caption{Protection Domains}
  \label{fig:prot_domains}
\end{figure}


The domain protection model does not address all possible memory corruption
faults; for example,
% 
modules can still corrupt their own state.
% 
This form of corruption, though undesirable, is less serious than
corruption across domains, and stable kernel can always ensure a clean
re-start of user modules when corruption is detected.
% 
On other hand, a corrupted kernel has truly unpredictable behavior,
leaving complete system reboot through a watchdog or grenade timer as
the only possible means of recovery~\cite{dutta05ipsn}.
%


Creating and enforcing protection domains is a challenging task on
resource constrained embedded platforms (Section~\ref{subsec:sfi})
%
% Initial SFI designs allowed a sandboxed module to access a single
% contiguous range of memory~\cite{wahbe93sfi}.
% %
% Motes' limited physical memory and absence
% of virtual memory precludes this partitioning; such a partition would
% constrain applications by severely limiting available memory and lead
% to internal fragmentation, extremely wasteful on severely resource
% constrained platforms.
%
%% Static contiguous partitioning is only possible in desktop systems
%% with large address spaces~\cite{wahbe93sfi}.
%
Harbor's \emph{memory map} abstraction was designed with the following
requirements:
%
first, it should have a small and customizable memory footprint;
%
second, it should permit arbitrary layout of state within the data memory;
%
and third, it should be easy to incorporate into existing
operating systems.
%
We propose a design that satisfies these requirements.
%
%-----------------------------------------------------------------
\subsection{Data Structure}
%
We assume a sensor node's address space is partitioned by the operating
system into small, contiguous \emph{blocks} of equal size, then allocated
to domains in \textit{segments} consisting of sets of contiguous blocks.
%
(On Atmel ATMEGA128~\cite{avrdatasheet}, SOS's block size is 8~bytes.)
%
The allocation of segments to domains could be static (at compile
time) or dynamic (through malloc).
%
A domain could be allocated multiple segments that are scattered
randomly across entire address space.
%
The Harbor memory map contains \emph{per-block} access permissions for
the entire address space.
%
The main operation of the memory map is to store and retrieve access
permissions for a given address.
%
Its design goal is to balance lookup efficiency and the extra storage
required for the permissions table.
%
The memory map contains ownership information (a domain identity) for every
block of memory, and encodes information about memory layout, such as
the start of a logical allocation segment.
%
The memory map must contain sufficient permission bits per block to encode
the total number of domains supported by the system.
%
Supporting two distinct domains (kernel and user) requires just one
domain bit per block, four domains require two domain bits per block,
and so forth.
%
Table~\ref{tab:mmap_table} shows an example of Harbor's memory map encoding
in a system with 8 domains.
%
\begin{table}[htdp]
\centering
\small{
\begin{tabular}{|c|l|}
	\hline
	Code & Meaning\\
	\hline
	1111 & Free, or start of kernel allocated segment\\
	1110 & Later portion of kernel allocated segment\\
	xxx1 & Start of user allocated segment\\
	xxx0 & Later portion of user allocated segment\\
	\hline
\end{tabular}}
\caption{Memory map information encoding for 8-domain protection.}
\label{tab:mmap_table}
\end{table}

%
The memory map is a configurable data structure; tradeoffs between
the inter-module protection and memory map size are discussed in
Section~\ref{sec:mmapdesignspace}.
%
%The design of memory map uses two bits of encoded information per
%block to provide memory protection.
%
%Due to the severe memory resource constraints, the memory map is
%designed to have a very small memory footprint. 
%
%The memory map is organized as a byte array where each byte is packed
%to store permission information about multiple blocks.
%
%Such an organization was chosen for maximum storage efficiency and
%minimum resource utilization.
%
%The memory protection mechanism intercepts every write operation of a
%user module to ensure that the permissions are not violated.
%
%The main task of the memory map manager is to store or retrieve the
%permissions for any given address.
%
%--------------------------------
% \subsubsection{Address Translation}
% \label{sec:memtrans}
% %
% Figure~\ref{fig:addr_memmap_translate} shows how an address is looked up in
% the memory map.
% %
% Assuming a block size of 8 bytes, the last three bits of address are an
% offset into a given block.
% %
% The remaining bits of the address represent a block number in data memory.
% %
% Access permissions are packed into a byte.
% %
% If encoded information is stored in four bits (for 8-domain
% protection), then each byte would contain information for two
% contiguous memory blocks.
% %
% Therefore, the last bit of block number selects a memory map record
% from within an access permissions byte.
% %
% The remaining bits of the block number form an index into the memory map table.
% %
% This design was chosen to minimize memory footprint.
% %
% \begin{figure}[htbp]
%   \centering
%   \includegraphics[height=1.5in,
%   keepaspectratio=true]{figures/memaddrtrans.eps}
%   \caption{Address to memory map translation (8-domain mode)}
%   \label{fig:addr_memmap_translate}
% \end{figure}
%
%------------------------------
% \subsubsection{Memory Map API}
% %
% The memory map data structure and address translation operations are
% encapsulated in an object accessible through the API described
% in Table~\ref{tab:memmap_api}. 	
% %
% %
% % If we optimize the memory map table to not include certain portions
% % of the address space, then what will happen if we accidentally pass
% % an unmapped address ?
% %
% %
% %
% \begin{table}[htdp]
%    \centering
%    \small{
%    \begin{tabular}{|l|}
%    \hline
%    Prototype and Description \\
%    \hline
%    \texttt{int8\_t memmap\_set(uint8\_t blkID, uint8\_t nBlks, uint8\_t
%    domID)}\\
%    Set owner of segment [BlkID, BlkID + nBlks) to domID \\
%    \hline
%    \texttt{uint8\_t memmap\_get(uint8\_t blkID)}\\
%    Get owner and layout of block number BlkID \\
%    \hline
%    \end{tabular}
%    }
%    \centering
%    \caption{Memory map API}
%    \label{tab:memmap_api}
% \end{table}
% %
%
%========================================================================================================================================
% MEMORY MAP CHECKER
%========================================================================================================================================
\subsection{Memory Map Checker}
\label{sec:mmapchecker}
%
Harbor's run-time checks validate memory accesses, in particular writes.
%
These accesses are validated using a protection model.
%
Our memory map checker enforces the protection model described earlier:
each user module can write only into its own domain.
%
% Further, a single trusted domain in the system is allowed to access all memory.
%
The memory map checker belongs to the trusted domain.
%
%A run-time checker restricts the memory access of user modules to permissible regions.
%
%The access control permissions are stored and tracked by the memory map manager.
%
%The policy used for access control can vary.
%
%The most common policy is to prevent the user modules from ever writing to a memory region that is owned by the kernel.
%
%The modules are instrumented to introduce checks before every write operation that needs protection.
%
%   // Get permissions through bit shifts
%   // uint8_t perms_bm = (BLOCK_TYPE_BM << ((mmap_offset >> 3) << 1));
Pseudocode for a write access check in a system with 8-domain protection is shown in Figure~\ref{fig:checker}.
%
\begin{figure} [htbp]
  \centering
  \begin{tiny}
\begin{verbatim}
write_access_check(addr_t addr, data_t data) {
   // Check is for writes outside stack region
   if (addr < STACK_PTR) {
      // Address translation: Get table index
      uint16_t blk_num = (addr >> log2_blk_size);
      uint16_t mmap_index = (blk_num >> log2_rec_per_byte);
      // Retrieve memory map byte
      uint8_t mmap_byte = MEM_MAP_PERMS_TBL[mmap_index];
  
      // Get the appropriate record in byte
      if (blk_num & SWAP_MASK) swap(mmap_byte);
      uint8_t mmap_owner = mmap_byte & OWNER_MASK;
      uint8_t first_blk_in_segment = mmap_byte & LAYOUT_MASK;

      // Validate access
      if (mmap_owner != curr_dom_id
          || (first_blk_in_segment && addr points to block metadata))
         mem_access_exception();

      // Perform store
      st addr, data;
   } else {
      // Check for writes to stack 
      stack_access_check(addr, data);
   } 
}
\end{verbatim}
      \end{tiny}
  \vskip-\baselineskip
  \caption{Pseudocode for Memory Map Checker (8-domain protection)}
  \label{fig:checker}
\end{figure}
%

The write access checker performs three operations.
%
First, it performs address translation to retrieve the byte containing
ownership information from the memory map table for a given address.
%
Second, it locates the appropriate record within that byte and determines
the domain of the block's owner.
%generates a bit mask from the address offset to derive the actual permission.
%
Third, it compares this domain ID and the current executing user module's domain ID.
%
A store is allowed only if these domains match.
%
As mentioned previously, the memory map manager does not maintain
permissions for run-time stack;
%
write accesses to the run-time stack are subject to a different
check described in Section~\ref{subsec:stackguard}.
%

%
% \begin{figure}[htpb]
%  \centering
%   \mbox{
%     \subfigure[Memory map byte for two
%     domains]{\label{fig:memmaptwodoms}\includegraphics[width=2in,
%       keepaspectratio = true]{figures/memmaptwodoms.eps}}
%     \hspace{0.2in}
%     \subfigure[Bit-Mask Lookup
%     Table]{\label{fig:perm_lut}\includegraphics[height=1.5in,
%       keepaspectratio = true]{Figures/perms_lut_opt.eps}}
%   }
%   \caption{Memory Map Checker Optimizations (2-domains)}
% \end{figure} 

%
% \begin{figure}[htbp]
%   \centering
%    \includegraphics[height=1in, keepaspectratio=true]{figures/perms_lut_opt.jpg} 
%    \caption{Lookup table optimization to implement bit-shift operations}
%    \label{fig:perm_lut}
% \end{figure}
%
%-----------------------------------------
% \subsubsection{Bit-Mask Lookup Table}
% \label{sec:bitmasklut}
% %
% Next we discuss a few implementation details in the design of
% memory map.
% %
% The memory map checker described in Figure~\ref{fig:checker} is
% designed for 8 protection domains.
% %
% The checker operation can be further optimized for a system with only 2
% protection domains.
% %
% In a 2 domain system, the \texttt{curr\_dom\_id} is always going to be
% the untrusted domain, because checks are introduced only in the
% untrusted domain.
% %
% Hence a store is valid if and only if the memory block belongs to the untrusted
% domain.
% %
% A memory map record for 2-domain protection requires only 2 bits.
% %
% Therefore, 4 memory map records can be efficiently packed into a byte
% as shown in Figure~\ref{fig:memmaptwodoms}.
% %
% The bit-mask required to retrieve the correct memory map record is
% generated through complex bit shift operations as shown in
% Figure~\ref{fig:perm_lut}.
% %
% The operation takes 32 clock cycles on ATMEGA128L as there is no
% instruction level support for arbitrary bit shifts.
% %
% Therefore, we use a lookup table stored in flash memory to retrieve
% the bit-masks.
% %
% Organization of the lookup table and its operation is described in
% Figure~\ref{fig:perm_lut}.
% %
% The table lookup takes only 8 clock cycles.
% %
% %--------------------------------------------------
% \subsubsection{Memory Heap Metadata}
% %
% %% This subsection describes a practical issue that is faced while using
% %% memory map with heaps used for dynamic memory allocation.
% %
% An implementation detail involves the protection of heap metadata, such as
% the owner and/or size of an allocated segment.
% %
% SOS stores this metadata in the segment itself as shown in
% Figure~\ref{fig:sos_free_list}.
% %
% This complicates the write access check, since heap metadata is
% effectively kernel-domain information and must be protected from wild
% writes.
% %
% %% This is unlike desktop systems where each process has its own heap.
% %
% %% Heap implementations store metadata information in data-structures
% %% that are embedded within allocated memory.
% %
% %For example, SOS kernel implements ownership tracking of memory
% %segments.
% %
% %During module unloading, kernel uses the ownership information to
% %free all memory owned by that module.
% %
% Since the SOS kernel stores this metadata in a segment's first
% memory block, %% as shown in Figure~\ref{fig:sos_free_list}.
% %
% %% This organization enables the heap to efficiently reclaim memory that
% %% is freed.
% %
% %The segment size information is used to implement the dynamic memory
% %allocate and free routines.
% %
% %% However, modules can overwrite the metadata, as it lies within the block
% %% boundary.
% %% %
% %% Recall that permissions are stored only at block granularity.
% %
% the memory map checker protects the metadata in
% the first block of any segment from user writes.
% %
% The memory map table's layout information supports this check by
% identifying the starting block of any segment.
% %
% The metadata checks are also implemented using the lookup table for
% improved efficiency.
% %
% \begin{figure} [h]
%   \centering
%     {\small
% \begin{verbatim}
% typedef struct _Block {
%    uint16_t segmentSize;
%    union {
%       uint8_t userPart[BLOCK_SIZE - sizeof(uint16_t)];
%       struct {
%          struct _Block *prev;
%          struct _Block *next;
%       };
%    };
% } Block;
% \end{verbatim} }
% \caption{SOS block metadata}
% \label{fig:sos_free_list}
% \end{figure}
%
%Checks are introduced by a re-write of binary generated by cross-compiler tool-chain.
%
%We describe design of binary rewriter in Subsection~\ref{sec:writeverify}
%
%The CIL (C Intermediate Language)~\cite{cil02necula} framework catches all the writes made by the user module and inserts the appropriate write access check.
%
%In future, the checks would be introduced by an automatic binary re-write of the user modules.
%
%The binary re-writes would be performed at the load time of the
%modules into the sensor network.
%========================================================================================================================================
% MEMORY MAP FOR PROTECTION
%========================================================================================================================================
\subsection{Using the Memory Map for Protection}
\label{sec:mmap_for_protection}
%
Information stored in the memory map can be used for a variety of protection models;
%
our protection model restricts programs from writing to memory outside their domain.
%
%% We discuss how memory map can be used in any system to enforce this protection model.
%


%
Systems using a memory map need to ensure the following four conditions.
%
First, the memory map should accurately reflect the current ownership and
layout of memory.
%
In any real system, memory is constantly allocated, freed, and/or
transferred from one module to another.
%
The memory map should be immediately updated when any of these events
occur; thus, SOS's
%
\texttt{malloc}, \texttt{free}, and
\texttt{change\_own} system calls were modified to update the memory
map data structure. 
%
Second, only the block owner should be permitted to free or change its ownership.
%
This condition is necessary as one module may accidentally (due to
programming errors) attempt to free memory being used by other
modules.
%
%% It prevents a module from accidentally hijacking memory that is
%% owned by other modules.
%
To enforce this condition, the system needs to track the currently
active domain (Section~\ref{sec:cfmgr}).
%
%
Third, direct access to the memory map API %(described in Table~\ref{tab:memmap_api})
should be restricted to trusted domains, such as the kernel.
%
%% If modules are allowed direct access to memory map API, then we can no
%% longer trust information stored in it.
%% %
%% Programming bugs can cause incorrect parameters to be passed to memory map API.
%% %
%% In SOS, kernel is treated as a trusted domain and is allowed access to memory map API.
%
%% User modules are prevented access by restricting their control flow.
%
In addition, the blocks storing memory map data structures should be owned
by a trusted domain, preventing
%
accidental corruption of the memory map data structure.
%


%The memory map manager tracks the permission information for every block in the address space of the sensor node.
%
A memory map can be easily incorporated into software systems.
%
As an example, we describe how SOS's memory map provides multi-domain protection.
%
%Memory map Two domain protection in SOS is easily implemented.
%
The memory map is initialized such that all statically allocated kernel
memory blocks are marked as owned by kernel.
%
%% Statically allocated blocks are used exclusively by kernel only and
%% user modules never read or write to them.
%
The remaining portion of the address space is partitioned into a heap,
a safe stack (further described in Section~\ref{subsec:safe_stack}), and a run-time stack.
%
The heap is divided into blocks, so the minimum granularity of 
%%  and a set of contiguous blocks (segments)
%% are dynamically allocated to user modules or the kernel upon request.
%
memory allocation is a block.
%
% Mention something about the choice of block size
%
The heap's memory map is initially marked as free.
%
The safe stack is marked as belonging to the kernel domain.
%
The run-time stack has no memory map;
%
we discuss run-time stack protection in Section~\ref{subsec:stackguard}.
%
Our implementation modified 150 lines of code, about 1\% of the 12720-line
SOS kernel; the change was
%
mostly localized to
dynamic memory management routines. 
%
%
% What are the implications of over-writing the stack ?
%
%The memory map manager works closely with the dynamic memory manager in the SOS operating system.
%
%Any request for dynamic memory is passed to the memory map manager that sets the correct permissions for the set of allocated blocks.
%
%During free operation, the memory map manager automatically clears the permissions.
%
%The dynamic memory manager in SOS permits the ownership change for dynamically allocated memory blocks.
%
%The memory map manager tracks any changes to the permissions that are caused due to the ownership transfer of a set of memory blocks.
%
%
%
%The dynamic memory allocator in SOS maintains a free list of unused memory blocks.
%
%Figure~\ref{fig:sos_free_list} shows the data structure implementing a memory block in SOS kernel.
%
%The data structure contains metadata that stores the number of contiguous blocks constituting the current segment.
%
%This information is critical for the correct functioning of the dynamic memory allocator.
%
%However, as the metadata is a part of the memory block, it can be easily corrupted by the user module.
%
%This is because the protection is provided only at the block level granularity.
%
%The problem was solved by eliminating the metadata information from
%the block structure and deriving it at run-time based upon the layout
%information stored in the memory map table.
%
%The absence of the metadata impacts the execution overhead of the dynamic memory operations.
%
%xs
%--------------------------------------------------------------------------------
\subsection{Memory Map Design Tradeoffs}
\label{sec:mmapdesignspace}
%
Harbor provides a number of design knobs that allow systems to trade
off protection and resource utilization.
%
First, the memory map data structure is configured by changing the
number of bits stored per block to match the number of domains
required by a system.
%
For example, two bits per block can create a two-domain system (a user/kernel
model) and four bits per block support eight protection
domains.
%
We have found eight domains to be sufficient for most sensor network
systems using SOS.
%
Increasing the number of bits stored per block increases the size of
the memory map.
%
%We have currently evaluated scenarios with only one SOS module per
%domain.
%
%However, Harbor does not pose this restriction.
%
%Systems have freedom to group a set of modules and assign them to a
%single domain.
%
%The block size can be set based upon typical size of memory objects
%used in the system.
%

Second, there is a tradeoff between the size of the memory map and the
amount of memory fragmentation.
%
Larger block size leads to increased internal fragmentation, but
reduces the number of blocks and thereby the size of the memory map.
%
For example, a block size of 256 bytes is suitable for the large image
and matrix objects passed around in the Cyclops
imager~\cite{cyclops05sensys}.
%
Mica2 based modules use a block size of 8 bytes.
%
Harbor and SOS currently support a single fixed block size for the
entire heap; an extension might permit using different block
granularity in different memory regions.
%

Third, the memory map can be configured to track ownership and layout
in only a subset of the entire address space, reducing its space
overhead. 
%
%This reduces the size of memory map required for protection.
%
For example, in SOS the memory map tracks only the heap and safe
stack.
%
%Memory accesses made by user modules to addresses outside the heap
%(except those into run-time stack) are considered invalid.
%
%% This reduces the size of memory map by 33\% (See
%% Table~\ref{tab:kernel_size_comparison})
%
In general, the size of the memory map can be reduced by decreasing
the fraction of memory reserved for the heap.
% as opposed to run-time stack.
%
%Many software systems create custom heap implementations
%(e.g. Message pools in SNACK~\cite{ben04snack}) that cover only a
%fraction of total address space.
%
%\subsection{Summary}
%

This section explains the memory map, an efficient data structure that
records ownership and layout information for any region of the address
space.
%
It is easily configurable to tradeoff the level of protection
for the resources utilized.
%
Memory map along with the Control Flow Manager (described in the next
section) enables protection without static partitioning of the address
space in micro-controllers.



